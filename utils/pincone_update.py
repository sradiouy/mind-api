import os
import json
import time
from pinecone import Pinecone
from math import ceil
from collections import defaultdict

# === CONFIGURATION ===
# --- Pinecone Credentials & Index ---
# Best practice: Load API key from environment variable
PINECONE_API_KEY = ""
# Environment often not needed directly for client v3 connection but good for reference
PINECONE_ENVIRONMENT = "gcp-starter"
# Replace with the name of your Pinecone index
PINECONE_INDEX_NAME = "mind"
# Optional: Specify a namespace if you use them
PINECONE_NAMESPACE = "" # e.g., "pubmed-data" or leave empty for default namespace

# --- Input Data ---
# Path to the JSON file generated by the processing script
INPUT_JSON_FILE = "pinecone_data_final_modules.json" # Make sure this path is correct

# --- Processing Parameters ---
BATCH_SIZE = 100 # Pinecone recommends up to 100 for upsert

# === SCRIPT ===

def load_data_from_json(filepath):
    """Loads the vector data from the specified JSON file."""
    try:
        with open(filepath, 'r') as f:
            data = json.load(f)
        print(f"Successfully loaded {len(data)} records from {filepath}")
        if not isinstance(data, list):
            raise ValueError("JSON data should be a list of records.")
        if data and not all(k in data[0] for k in ('id', 'values', 'metadata')):
             print("Warning: Records might be missing 'id', 'values', or 'metadata' keys.")
        # Ensure essential metadata like 'pmid' exists for filtering
        if data and 'pmid' not in data[0].get('metadata', {}):
            raise ValueError("Records in JSON must have 'pmid' in their metadata for filtering.")
        return data
    except FileNotFoundError:
        print(f"Error: Input JSON file not found at {filepath}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {filepath}")
        return None
    except ValueError as e:
        print(f"Error: {e}")
        return None
    except Exception as e:
        print(f"An error occurred while loading data: {e}")
        return None

def find_existing_pmids(index, data_records, namespace=""):
    """
    Checks Pinecone for the existence of PMIDs based on a proxy chunk ID.
    Returns a set of PMIDs that already seem to exist.
    """
    print("\nüîç Checking for existing PMIDs in Pinecone...")
    pmid_to_first_id = {}
    # Find the first occurring Pinecone ID for each unique PMID in the input data
    for record in data_records:
        pmid = record.get('metadata', {}).get('pmid')
        if pmid and pmid not in pmid_to_first_id:
            pmid_to_first_id[pmid] = record['id'] # Use the actual ID from the JSON

    existing_pmids = set()
    pmids_to_check = list(pmid_to_first_id.keys())
    ids_to_fetch = list(pmid_to_first_id.values())

    # Fetch proxy IDs in batches
    fetch_batch_size = 1000 # Can fetch many IDs at once
    for i in range(0, len(ids_to_fetch), fetch_batch_size):
        batch_fetch_ids = ids_to_fetch[i:i + fetch_batch_size]
        corresponding_pmids = pmids_to_check[i:i + fetch_batch_size]
        print(f"  Fetching batch {i//fetch_batch_size + 1} to check {len(batch_fetch_ids)} PMIDs...")
        try:
            fetch_response = index.fetch(ids=batch_fetch_ids, namespace=namespace)
            found_vector_ids = set(fetch_response.vectors.keys())

            # Map found vector IDs back to PMIDs
            for pmid, proxy_id in zip(corresponding_pmids, batch_fetch_ids):
                if proxy_id in found_vector_ids:
                    existing_pmids.add(pmid)

        except ApiException as e:
            print(f"  ‚ö†Ô∏è Error fetching batch from Pinecone: {e}. PMIDs in this batch may be re-uploaded.")
        except Exception as e:
            print(f"  ‚ö†Ô∏è An unexpected error occurred during fetch: {e}.")
        time.sleep(0.2) # Small delay between fetch batches

    print(f"Found {len(existing_pmids)} PMIDs potentially already in Pinecone (based on first chunk check).")
    return existing_pmids

def upsert_new_data(index, data_records, pmids_to_skip, namespace=""):
    """Filters out records belonging to skipped PMIDs and upserts the rest."""

    # Filter records
    records_to_upload = [
        record for record in data_records
        if record.get('metadata', {}).get('pmid') not in pmids_to_skip
    ]
    total_new_records = len(records_to_upload)

    if not total_new_records:
        print("\nNo new records to upload (all PMIDs found or input was empty).")
        return

    print(f"\nüöÄ Preparing to upload {total_new_records} chunks belonging to new PMIDs...")

    upserted_count = 0
    # Process in batches
    for i in range(0, total_new_records, BATCH_SIZE):
        batch_upload_records = records_to_upload[i : i + BATCH_SIZE]
        # Prepare for upsert format: list of tuples or Vector objects
        vectors_to_upsert = [
            (record['id'], record['values'], record['metadata'])
            for record in batch_upload_records
        ]

        print(f"  Uploading batch {i//BATCH_SIZE + 1}/{ceil(total_new_records/BATCH_SIZE)} ({len(vectors_to_upsert)} records)...")
        try:
            upsert_response = index.upsert(vectors=vectors_to_upsert, namespace=namespace)
            upserted_count += upsert_response.upserted_count
        except ApiException as e:
            print(f"  ‚ö†Ô∏è Error upserting batch: {e}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è An unexpected error occurred during upsert: {e}")
        time.sleep(0.5) # Small delay between upsert batches

    print("\n--- Upload Summary ---")
    print(f"Attempted to upload: {total_new_records} records")
    print(f"Successfully upserted count reported by Pinecone: {upserted_count}")
    print(f"(Skipped {len(data_records) - total_new_records} records belonging to existing PMIDs)")
    print("----------------------")


if __name__ == "__main__":
    print("Starting Pinecone upsert script (PMID check mode)...")

    # --- Validate Configuration ---
    if "YOUR_PINECONE_API_KEY" in PINECONE_API_KEY or not PINECONE_API_KEY:
        print("Error: Pinecone API Key not set. Please set the PINECONE_API_KEY environment variable or update the script.")
        exit(1)
    # Environment often not needed directly for client v3 connection
    # if "YOUR_PINECONE_ENVIRONMENT" in PINECONE_ENVIRONMENT or not PINECONE_ENVIRONMENT:
    #     print("Error: Pinecone Environment is not set.")
    #     exit(1)
    if "your-index-name-here" in PINECONE_INDEX_NAME or not PINECONE_INDEX_NAME:
        print("Error: Pinecone Index Name is not set. Please update PINECONE_INDEX_NAME in the script.")
        exit(1)

    # --- Load Data ---
    data = load_data_from_json(INPUT_JSON_FILE)
    if data is None:
        exit(1)
    if not data:
        print("Input JSON file contains no records. Exiting.")
        exit(0)

    # --- Initialize Pinecone ---
    # --- Initialize Pinecone ---
    try:
        print(f"Initializing Pinecone connection...")
        # Correct initialization for v3+ client
        pc = Pinecone(api_key=PINECONE_API_KEY)

        print(f"Checking if index '{PINECONE_INDEX_NAME}' exists...")

        # --- CORRECTED Index Existence Check ---
        index_list = pc.list_indexes() # Returns an IndexList object
        # Extract the names from the IndexDescription objects within the list
        index_names = [index_info.name for index_info in index_list.indexes]

        if PINECONE_INDEX_NAME not in index_names:
             print(f"Error: Index '{PINECONE_INDEX_NAME}' does not exist in the environment/project.")
             # You might want to add logic here to CREATE the index if desired
             # e.g., from pinecone import ServerlessSpec, PodSpec
             # pc.create_index(PINECONE_INDEX_NAME, dimension=YOUR_DIMENSION, metric='cosine', spec=ServerlessSpec(cloud='aws', region='us-west-2'))
             exit(1)
        # --- END Correction ---

        # Get the index object (this part was already correct)
        index = pc.Index(PINECONE_INDEX_NAME)
        print("Connection successful. Index stats:")
        # Fetching stats is also correct
        print(index.describe_index_stats())

    except ApiException as api_e: # Catch specific Pinecone API errors
        print(f"Pinecone API Error initializing or connecting to index: {api_e}")
        exit(1)
    except Exception as e: # Catch other potential errors during init
        print(f"Error initializing Pinecone or connecting to index: {e}")
        # The original TypeError likely happened before this point if it was during the check
        exit(1)
        
    # --- Check Existing PMIDs ---
    pmids_already_present = find_existing_pmids(index, data, namespace=PINECONE_NAMESPACE)

    # --- Upsert New Data ---
    upsert_new_data(index, data, pmids_already_present, namespace=PINECONE_NAMESPACE)

    print("\nScript finished.")